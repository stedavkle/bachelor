@misc{youtube-derbauer,
    title = {Unglaubliche Präzision - Wir untersuchen einen einzelnen TSMC 7nm Transistor. Kleindiek Teil 1/2},
    year = {2021},
    author = {Roman Hartung},
    howpublished = {\url{https://www.youtube.com/watch?v=zY5WC9kMS0g&t}},
    note = {Letzter Zugriff: \today}
}
@misc{kn,
        title = {Kleindiek Nanotechnik GmbH},
        year = {2023},
        author = {Stephan Kleindiek},
        howpublished = {\url{https://www.nanotechnik.com/}},
        note = {Letzter Zugriff: \today}
}
@misc{pooling,
        title = {Introduction To Pooling Layers In CNN},
        year = {2022},
        author = {Rafay Qayyum},
        howpublished = {\url{https://towardsai.net/p/l/introduction-to-pooling-layers-in-cnn}},
        note = {Letzter Zugriff: \today}
}
@misc{convolution,
        title = {Convolution, Padding, Stride, and Pooling in CNN},
        year = {2022},
        author = {Abhishek Kumar Pandey},
        howpublished = {\url{https://medium.com/analytics-vidhya/convolution-padding-stride-and-pooling-in-cnn-13dc1f3ada26}},
        note = {Letzter Zugriff: \today}
}
@misc{learnrate,
        title = {Understanding Learning Rate},
        year = {2019},
        author = {Aditya Rakhecha},
        howpublished = {\url{https://towardsdatascience.com/https-medium-com-dashingaditya-rakhecha-understanding-learning-rate-dd5da26bb6de}},
        note = {Letzter Zugriff: \today}
}
@misc{weisstein_convolution,
  author = {Eric W. Weisstein},
  title = {Convolution},
  year = {2023},
  howpublished = {\url{https://mathworld.wolfram.com/Convolution.html}},
  note = {Letzter Zugriff: \today}
}
@software{adobephotoshop,
  author = {{Adobe Inc.}},
  title = {Adobe Photoshop},
  url = {https://www.adobe.com/products/photoshop.html},
  version = {CC 2023},
  date = {2023},
} 
@misc{cuda,
  author={NVIDIA and Vingelmann, Péter and Fitzek, Frank H.P.},
  title={CUDA, release: 10.2.89},
  year={2020},
  url={https://developer.nvidia.com/cuda-toolkit},
}
@article{kanopoulos1988design,
  title={Design of an image edge detection filter using the Sobel operator},
  author={Kanopoulos, Nick and Vasanthavada, Nagesh and Baker, Robert L},
  journal={IEEE Journal of solid-state circuits},
  volume={23},
  number={2},
  pages={358--367},
  year={1988},
  publisher={IEEE}
} 
% Tracker
@inproceedings{Wojke2017simple,
  title={Simple Online and Realtime Tracking with a Deep Association Metric},
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
  year={2017},
  pages={3645--3649},
  organization={IEEE},
  doi={10.1109/ICIP.2017.8296962}
}
@inproceedings{Bewley2016_sort,
  author={Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
  title={Simple online and realtime tracking},
  year={2016},
  pages={3464-3468},
  keywords={Benchmark testing;Complexity theory;Detectors;Kalman filters;Target tracking;Visualization;Computer Vision;Data Association;Detection;Multiple Object Tracking},
  doi={10.1109/ICIP.2016.7533003}
}
% Paper
% This file was created with Citavi 6.15.2.0

@article{Rumelhart.1986,
 abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
 author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
 year = {1986},
 title = {Learning representations by back-propagating errors},
 pages = {533--536},
 volume = {323},
 number = {6088},
 issn = {1476-4687},
 journal = {Nature},
 doi = {10.1038/323533a0}
}



@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{AMARI1993185,
title = {Backpropagation and stochastic gradient descent method},
journal = {Neurocomputing},
volume = {5},
number = {4},
pages = {185-196},
year = {1993},
issn = {0925-2312},
doi = {https://doi.org/10.1016/0925-2312(93)90006-O},
url = {https://www.sciencedirect.com/science/article/pii/092523129390006O},
author = {Shun-ichi Amari},
keywords = {Stochastic descent, generalized delta rule, dynamics of learning, pattern classification, multilayer perceptron},
abstract = {The backpropagation learning method has opened a way to wide applications of neural network research. It is a type of the stochastic descent method known in the sixties. The present paper reviews the wide applicability of the stochastic gradient descent method to various types of models and loss functions. In particular, we apply it to the pattern recognition problem, obtaining a new learning algorithm based on the information criterion. Dynamical properties of learning curves are then studied based on an old paper by the author where the stochastic descent method was proposed for general multilayer networks. The paper is concluded with a short section offering some historical remarks.}
}
@article{DBLP:journals/corr/GirshickDDM13,
  author       = {Ross B. Girshick and
                  Jeff Donahue and
                  Trevor Darrell and
                  Jitendra Malik},
  title        = {Rich feature hierarchies for accurate object detection and semantic
                  segmentation},
  journal      = {CoRR},
  volume       = {abs/1311.2524},
  year         = {2013},
  url          = {http://arxiv.org/abs/1311.2524},
  eprinttype    = {arXiv},
  eprint       = {1311.2524},
  timestamp    = {Mon, 13 Aug 2018 16:48:09 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GirshickDDM13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
% Convolution
@inproceedings{yu2015multi,
  title     = {Multi-scale context aggregation by dilated convolutions},
  author    = {Yu, Fisher and Koltun, Vladlen},
  booktitle = {International Conference on Learning Representations},
  year      = {2016}
}
% RCNN
@ARTICLE{7112511,
  author={Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Region-Based Convolutional Networks for Accurate Object Detection and Segmentation}, 
  year={2016},
  volume={38},
  number={1},
  pages={142-158},
  doi={10.1109/TPAMI.2015.2437384}}
@InProceedings{Girshick_2015_ICCV,
author = {Girshick, Ross},
title = {Fast R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
} 
@article{DBLP:journals/corr/RenHG015,
  author       = {Shaoqing Ren and
                  Kaiming He and
                  Ross B. Girshick and
                  Jian Sun},
  title        = {Faster {R-CNN:} Towards Real-Time Object Detection with Region Proposal
                  Networks},
  journal      = {CoRR},
  volume       = {abs/1506.01497},
  year         = {2015},
  url          = {http://arxiv.org/abs/1506.01497},
  eprinttype    = {arXiv},
  eprint       = {1506.01497},
  timestamp    = {Mon, 13 Aug 2018 16:46:02 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/RenHG015.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@InProceedings{He_2017_ICCV,
author = {He, Kaiming and Gkioxari, Georgia and Dollar, Piotr and Girshick, Ross},
title = {Mask R-CNN},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
} 

@InProceedings{Wang_2023_CVPR,
    author    = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
    title     = {YOLOv7: Trainable Bag-of-Freebies Sets New State-of-the-Art for Real-Time Object Detectors},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {7464-7475}
}
% Backbones
@InProceedings{Lin_2017_CVPR,
author = {Lin, Tsung-Yi and Dollar, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
title = {Feature Pyramid Networks for Object Detection},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}
@InProceedings{Yu_2017_CVPR,
author = {Yu, Fisher and Koltun, Vladlen and Funkhouser, Thomas},
title = {Dilated Residual Networks},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}
@InProceedings{He_2016_CVPR,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title = {Deep Residual Learning for Image Recognition},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}
@ARTICLE{279181,
  author={Bengio, Y. and Simard, P. and Frasconi, P.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning long-term dependencies with gradient descent is difficult}, 
  year={1994},
  volume={5},
  number={2},
  pages={157-166},
  doi={10.1109/72.279181}
}
@misc{1701.09175,
Author = {A. Emin Orhan and Xaq Pitkow},
Title = {Skip Connections Eliminate Singularities},
Year = {2017},
Eprint = {arXiv:1701.09175},
}
@misc{1603.05027,
Author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
Title = {Identity Mappings in Deep Residual Networks},
Year = {2016},
Eprint = {arXiv:1603.05027},
}
@misc{1507.06228,
Author = {Rupesh Kumar Srivastava and Klaus Greff and Jürgen Schmidhuber},
Title = {Training Very Deep Networks},
Year = {2015},
Eprint = {arXiv:1507.06228},
}
@misc{2206.08016,
Author = {Omar Elharrouss and Younes Akbari and Noor Almaadeed and Somaya Al-Maadeed},
Title = {Backbones-Review: Feature Extraction Networks for Deep Learning and Deep Reinforcement Learning Approaches},
Year = {2022},
Eprint = {arXiv:2206.08016},
}
@misc{1611.10012,
Author = {Jonathan Huang and Vivek Rathod and Chen Sun and Menglong Zhu and Anoop Korattikara and Alireza Fathi and Ian Fischer and Zbigniew Wojna and Yang Song and Sergio Guadarrama and Kevin Murphy},
Title = {Speed/accuracy trade-offs for modern convolutional object detectors},
Year = {2016},
Eprint = {arXiv:1611.10012},
}
@misc{1612.03144,
Author = {Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie},
Title = {Feature Pyramid Networks for Object Detection},
Year = {2016},
Eprint = {arXiv:1612.03144},
}


% SEM
@incollection{INKSON201617,
title = {2 - Scanning electron microscopy (SEM) and transmission electron microscopy (TEM) for materials characterization},
editor = {Gerhard Hübschen and Iris Altpeter and Ralf Tschuncky and Hans-Georg Herrmann},
booktitle = {Materials Characterization Using Nondestructive Evaluation (NDE) Methods},
publisher = {Woodhead Publishing},
pages = {17-43},
year = {2016},
isbn = {978-0-08-100040-3},
doi = {https://doi.org/10.1016/B978-0-08-100040-3.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100040300002X},
author = {B.J. Inkson},
keywords = {Backscattered electrons (BSEs), Electron diffraction, Electron energy loss spectroscopy (EELS), High-resolution, Microstructure, Scanning electron microscopy (SEM), Scanning transmission electron microscopy (STEM), Secondary electrons (SEs), Transmission electron microscopy (TEM), X-ray spectroscopy},
abstract = {Scanning electron microscopy (SEM) and transmission electron microscopy (TEM) are highly versatile methodologies for 2D and 3D materials characterization. The high spatial resolution of SEM and TEM, from nano- to microscale in both imaging and chemical characterization modes, is highly complementary to other nondestructive materials characterization techniques covered later in this book. In this chapter we cover the basic principles underpinning the use of electron microscopy, and give an overview of the core methodologies available in SEM and TEM. The range of useful signals generated by electron–matter interactions are described, and related to the SEM and TEM techniques that make use of them. The application SEM and TEM to the evaluation of key microstructural features of materials including surface topography, grain size, and local chemistry is discussed, and an overview of topical applications given.}
}
@Inbook{Zhou2007,
author="Zhou, Weilie
and Apkarian, Robert
and Wang, Zhong Lin
and Joy, David",
editor="Zhou, Weilie
and Wang, Zhong Lin",
title="Fundamentals of Scanning Electron Microscopy (SEM)",
bookTitle="Scanning Microscopy for Nanotechnology: Techniques and Applications",
year="2007",
publisher="Springer New York",
address="New York, NY",
pages="1--40",
isbn="978-0-387-39620-0",
doi="10.1007/978-0-387-39620-0_1",
url="https://doi.org/10.1007/978-0-387-39620-0_1"
}
@misc{geminisem,
  title = {ZEISS GeminiSEM},
  author = {{ZEISS}},
  year = {2023},
  howpublished = {\url{https://www.zeiss.com/microscopy/de/produkte/sem-und-fib-sem/sem/die-geminisem-produktfamilie.html}},
  urldate = {20/07/2023},
  note = {Letzter Zugriff: \today}
}

% Related Works
@INPROCEEDINGS{8512393,
  author={Liu, Jing and Li, Weifu and Xiao, Chi and Hong, Bei and Xie, Qiwei and Han, Hua},
  booktitle={2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)}, 
  title={Automatic Detection and Segmentation of Mitochondria from SEM Images using Deep Neural Network}, 
  year={2018},
  volume={},
  number={},
  pages={628-631},
  doi={10.1109/EMBC.2018.8512393}
}

@Article{vonChamier2021,
author={von Chamier, Lucas and Laine, Romain F. and Jukkala, Johanna and Spahn, Christoph and Krentzel, Daniel and Nehme, Elias and Lerche, Martina and Hern{\'a}ndez-P{\'e}rez, Sara and Mattila, Pieta K. and Karinou, Eleni and Holden, S{\'e}amus and Solak, Ahmet Can and Krull, Alexander and Buchholz, Tim-Oliver and Jones, Martin L. and Royer, Lo{\"i}c A. and Leterrier, Christophe and Shechtman, Yoav and Jug, Florian and Heilemann, Mike and Jacquemet, Guillaume and Henriques, Ricardo},
title={Democratising deep learning for microscopy with ZeroCostDL4Mic},
journal={Nature Communications},
year={2021},
month={Apr},
day={15},
volume={12},
number={1},
pages={2276},
abstract={Deep Learning (DL) methods are powerful analytical tools for microscopy and can outperform conventional image processing pipelines. Despite the enthusiasm and innovations fuelled by DL technology, the need to access powerful and compatible resources to train DL networks leads to an accessibility barrier that novice users often find difficult to overcome. Here, we present ZeroCostDL4Mic, an entry-level platform simplifying DL access by leveraging the free, cloud-based computational resources of Google Colab. ZeroCostDL4Mic allows researchers with no coding expertise to train and apply key DL networks to perform tasks including segmentation (using U-Net and StarDist), object detection (using YOLOv2), denoising (using CARE and Noise2Void), super-resolution microscopy (using Deep-STORM), and image-to-image translation (using Label-free prediction - fnet, pix2pix and CycleGAN). Importantly, we provide suitable quantitative tools for each network to evaluate model performance, allowing model optimisation. We demonstrate the application of the platform to study multiple biological processes.},
issn={2041-1723},
doi={10.1038/s41467-021-22518-0},
url={https://doi.org/10.1038/s41467-021-22518-0}
}
@article{Frei_2021,
	doi = {10.1016/j.powtec.2020.08.034},
	url = {https://doi.org/10.1016%2Fj.powtec.2020.08.034},
	year = 2021,
	month = {jan},
	publisher = {Elsevier {BV}},
	volume = {377},
	pages = {974--991},
	author = {M. Frei and F.E. Kruis},
	title = {{FibeR}-{CNN}: Expanding Mask R-{CNN} to improve image-based fiber analysis},
	journal = {Powder Technology}
}
@article{atoum2023adaptive,
  title={Adaptive Rectified Linear Unit (Arelu) for Classification Problems to Solve Dying Problem in Deep Learning},
  author={Atoum, I. A.},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={14},
  number={2},
  pages={97-102},
  year={2023},
  publisher={The Science and Information Organization},
  doi={10.14569/ijacsa.2023.0140212},
  url={http://thesai.org/Downloads/Volume14No2/Paper_12-Adaptive_Rectified_Linear_Unit.pdf}
}


% Tools
@misc{coco,
  title = {COCO Dataformat},
  author = {{COCO Consortium}},
  year = {2023},
  howpublished = {\url{https://cocodataset.org}},
  urldate = {20/07/2023},
  note = {Letzter Zugriff: \today}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@online{bioimage-datasets,
  title = {BioImage.IO datasets},
  author = {AI4Life},
  year = {2023},
  howpublished = {\url{https://bioimage.io/#/?type=dataset}},
  urldate = {20/07/2023},
  note = {Letzter Zugriff: \today}
}
@misc{paperswithcode-datasets,
  title = {Papers with Code - Datasets},
  author = {{Papers with Code}},
  year = {2023},
  howpublished = {\url{https://paperswithcode.com/datasets}},
  urldate = {20/07/2023},
  note = {Letzter Zugriff: \today}
}
@misc{paperswithcode-compvis,
  title = {Papers with Code - Computer Vision},
  author = {{Papers with Code}},
  year = {2023},
  howpublished = {\url{https://paperswithcode.com/area/computer-vision}},
  urldate = {20/07/2023},
  note = {Letzter Zugriff: \today}
}

@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019},
  note = {Letzter Zugriff: \today}
}
@misc{fiberrcnn,
  author =       {Max Frei},
  title =        {FibeR-CNN},
  howpublished = {\url{https://github.com/maxfrei750/FibeR-CNN}},
  year =         {2022},
  note = {Letzter Zugriff: \today}
}
@misc{mathworks-maskrcnn,
  title = {Getting Started with Mask R-CNN for Instance Segmentation},
  author = {MathWorks},
  year = {2023},
  howpublished = {\url{https://de.mathworks.com/help/vision/ug/getting-started-with-mask-r-cnn-for-instance-segmentation.html}},
  urldate = {15/07/2023},
  note = {Letzter Zugriff: \today}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal= {arXiv preprint arXiv:1906.07155},
  year={2019}
}
@ARTICLE{2020SciPy-NMeth,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  adsurl  = {https://rdcu.be/b08Wh},
  doi     = {10.1038/s41592-019-0686-2},
}
@misc{clark2015pillow,
  title={Pillow (PIL Fork) Documentation},
  author={Clark, Alex},
  year={2015},
  publisher={readthedocs},
  url={https://buildmedia.readthedocs.org/media/pdf/pillow/latest/pillow.pdf}
}
@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}
@misc{chriscoco,
  author = {Chris Eijgenstein},
  title = {image-to-coco-json-converter},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/chrise96/image-to-coco-json-converter}},
  commit = {76fc825a34376a796f451d5485bb5f49c0e10885},
  note = {Letzter Zugriff: \today}
}
@Misc{shapely2007,
  author = {Sean Gillies and others},
  organization = {toblerity.org},
  title = {Shapely: manipulation and analysis of geometric objects},
  year =  {2007--},
  url = "https://github.com/Toblerity/Shapely"
}
@article{van2014scikit,
  title={scikit-image: image processing in Python},
  author={Van der Walt, Stefan and Sch{\"o}nberger, Johannes L and Nunez-Iglesias, Juan and Boulogne, Fran{\c{c}}ois and Warner, Joshua D and Yager, Neil and Gouillart, Emmanuelle and Yu, Tony},
  journal={PeerJ},
  volume={2},
  pages={e453},
  year={2014},
  publisher={PeerJ Inc.}
}
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@misc{jetson,
  title        = {NVIDIA Jetson},
  author = {{NVIDIA}},
  howpublished = {\url{https://developer.nvidia.com/embedded/faq}},
  urldate = {25/07/2023},
  note = {Letzter Zugriff: \today}
}
@misc{tensorrt,
  title        = {NVIDIA TensorRT},
  author = {{NVIDIA}},
  howpublished = {\url{https://developer.nvidia.com/tensorrt}},
  urldate = {25/07/2023},
  note = {Letzter Zugriff: \today}
}
@misc{onnxruntime,
  title={ONNX Runtime},
  author={{ONNX Runtime developers}},
  year={2021},
  howpublished={\url{https://onnxruntime.ai/}},
  note = {Letzter Zugriff: \today}
}
@misc{google-colab,
  title        = {Colaboratory},
  author = {{Google Inc.}},
  howpublished = {\url{https://research.google.com/colaboratory/faq.html}},
  year = {2023},
  note = {Letzter Zugriff: \today}
}


% Paper
@inproceedings{SunXLW19,
  title={Deep High-Resolution Representation Learning for Human Pose Estimation},
  author={Ke Sun and Bin Xiao and Dong Liu and Jingdong Wang},
  booktitle={CVPR},
  year={2019}
}
@article{wang2019deep,
  title={Deep High-Resolution Representation Learning for Visual Recognition},
  author={Wang, Jingdong and Sun, Ke and Cheng, Tianheng and Jiang, Borui and Deng, Chaorui and Zhao, Yang and Liu, Dong and Mu, Yadong and Tan, Mingkui and Wang, Xinggang and Liu, Wenyu and Xiao, Bin},
  journal={TPAMI},
  year={2019}
}
@article{Ying_2019,
doi = {10.1088/1742-6596/1168/2/022022},
url = {https://dx.doi.org/10.1088/1742-6596/1168/2/022022},
year = {2019},
month = {feb},
publisher = {IOP Publishing},
volume = {1168},
number = {2},
pages = {022022},
author = {Xue Ying},
title = {An Overview of Overfitting and its Solutions},
journal = {Journal of Physics: Conference Series},
abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}
@article{doi:10.1021/ci0342472,
author = {Hawkins, Douglas M.},
title = {The Problem of Overfitting},
journal = {Journal of Chemical Information and Computer Sciences},
volume = {44},
number = {1},
pages = {1-12},
year = {2004},
doi = {10.1021/ci0342472},
note ={PMID: 14741005},
URL = {https://doi.org/10.1021/ci0342472
},
eprint = {https://doi.org/10.1021/ci0342472}
}
@misc{1706.02677,
Author = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
Title = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
Year = {2017},
Eprint = {arXiv:1706.02677},
}
@misc{1908.03265,
Author = {Liyuan Liu and Haoming Jiang and Pengcheng He and Weizhu Chen and Xiaodong Liu and Jianfeng Gao and Jiawei Han},
Title = {On the Variance of the Adaptive Learning Rate and Beyond},
Year = {2019},
Eprint = {arXiv:1908.03265},
}
@article{APICELLA202114,
title = {A survey on modern trainable activation functions},
journal = {Neural Networks},
volume = {138},
pages = {14-32},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.026},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021000344},
author = {Andrea Apicella and Francesco Donnarumma and Francesco Isgrò and Roberto Prevete},
keywords = {Neural networks, Machine learning, Activation functions, Trainable activation functions, Learnable activation functions},
abstract = {In neural networks literature, there is a strong interest in identifying and defining activation functions which can improve neural network performance. In recent years there has been a renovated interest in the scientific community in investigating activation functions which can be trained during the learning process, usually referred to as trainable, learnable or adaptable activation functions. They appear to lead to better network performance. Diverse and heterogeneous models of trainable activation function have been proposed in the literature. In this paper, we present a survey of these models. Starting from a discussion on the use of the term “activation function” in literature, we propose a taxonomy of trainable activation functions, highlight common and distinctive proprieties of recent and past models, and discuss main advantages and limitations of this type of approach. We show that many of the proposed approaches are equivalent to adding neuron layers which use fixed (non-trainable) activation functions and some simple local rule that constrains the corresponding weight layers.}
}
@article{KANDEL2020312,
title = {The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset},
journal = {ICT Express},
volume = {6},
number = {4},
pages = {312-315},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405959519303455},
author = {Ibrahem Kandel and Mauro Castelli},
keywords = {Convolutional neural networks, Deep learning, Image classification, Medical images, Batch size},
abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.}
}
@article{Hosang_2016,
	doi = {10.1109/tpami.2015.2465908},
	url = {https://doi.org/10.1109%2Ftpami.2015.2465908},
	year = 2016,
	month = {apr},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {38},
	number = {4},
	pages = {814--830},
	author = {Jan Hosang and Rodrigo Benenson and Piotr Dollar and Bernt Schiele},
	title = {What Makes for Effective Detection Proposals?},
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}
@article{Wang.2022,
 abstract = {As one of the important research topics in machine learning, loss function plays an important role in the construction of machine learning algorithms and the improvement of their performance, which has been concerned and explored by many researchers. But it still has a big gap to summarize, analyze and compare the classical loss functions. Therefore, this paper summarizes and analyzes 31 classical loss functions in machine learning. Specifically, we describe the loss functions from the aspects of traditional machine learning and deep learning respectively. The former is divided into classification problem, regression problem and unsupervised learning according to the task type. The latter is subdivided according to the application scenario, and here we mainly select object detection and face recognition to introduces their loss functions. In each task or application, in addition to analyzing each loss function from formula, meaning, image and algorithm, the loss functions under the same task or application are also summarized and compared to deepen the understanding and provide help for the selection and improvement of loss function.},
 author = {Wang, Qi and Ma, Yue and Zhao, Kun and Tian, Yingjie},
 year = {2022},
 title = {A Comprehensive Survey of Loss Functions in Machine Learning},
 pages = {187--212},
 volume = {9},
 number = {2},
 issn = {2198-5812},
 journal = {Annals of Data Science},
 doi = {10.1007/s40745-020-00253-5}
}


@InProceedings{pmlr-v119-cutkosky20b,
  title = 	 {Momentum Improves Normalized {SGD}},
  author =       {Cutkosky, Ashok and Mehta, Harsh},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2260--2268},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/cutkosky20b/cutkosky20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/cutkosky20b.html},
  abstract = 	 {We provide an improved analysis of normalized SGD showing that adding momentum provably removes the need for large batch sizes on non-convex objectives. Then, we consider the case of objectives with bounded second derivative and show that in this case a small tweak to the momentum formula allows normalized SGD with momentum to find an $\epsilon$-critical point in $O(1/\epsilon^{3.5})$ iterations, matching the best-known rates without accruing any logarithmic factors or dependence on dimension. We provide an adaptive learning rate schedule that automatically improves convergence rates when the variance in the gradients is small. Finally, we show that our method is effective when employed on popular large scale tasks such as ResNet-50 and BERT pretraining, matching the performance of the disparate methods used to get state-of-the-art results on both tasks.}
}

@InProceedings{pmlr-v28-sutskever13,
  title = 	 {On the importance of initialization and momentum in deep learning},
  author = 	 {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1139--1147},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/sutskever13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/sutskever13.html},
  abstract = 	 {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }
}
@article{Shah.2019,
 abstract = {Bone is an architecturally complex system that constantly undergoes structural and functional optimisation through renewal and repair. The scanning electron microscope (SEM) is among the most frequently used instruments for examining bone. It offers the key advantage of very high spatial resolution coupled with a large depth of field and wide field of view. Interactions between incident electrons and atoms on the sample surface generate backscattered electrons, secondary electrons, and various other signals including X-rays that relay compositional and topographical information. Through selective removal or preservation of specific tissue components (organic, inorganic, cellular, vascular), their individual contribution(s) to the overall functional competence can be elucidated. With few restrictions on sample geometry and a variety of applicable sample-processing routes, a given sample may be conveniently adapted for multiple analytical methods. While a conventional SEM operates at high vacuum conditions that demand clean, dry, and electrically conductive samples, non-conductive materials (e.g., bone) can be imaged without significant modification from the natural state using an environmental scanning electron microscope. This review highlights important insights gained into bone microstructure and pathophysiology, bone response to implanted biomaterials, elemental analysis, SEM in paleoarchaeology, 3D imaging using focused ion beam techniques, correlative microscopy and in situ experiments. The capacity to image seamlessly across multiple length scales within the meso-micro-nano-continuum, the SEM lends itself to many unique and diverse applications, which attest to the versatility and user-friendly nature of this instrument for studying bone. Significant technological developments are anticipated for analysing bone using the SEM.},
 author = {Shah, Furqan A. and Ruscs{\'a}k, Krisztina and Palmquist, Anders},
 year = {2019},
 title = {50 years of scanning electron microscopy of bone---a comprehensive overview of the important discoveries made and insights gained into bone material properties in health, disease, and taphonomy},
 pages = {15},
 volume = {7},
 number = {1},
 issn = {2095-6231},
 journal = {Bone Research},
 doi = {10.1038/s41413-019-0053-z}
}
@article{GARDNER19982627,
title = {Artificial neural networks (the multilayer perceptron)—a review of applications in the atmospheric sciences},
journal = {Atmospheric Environment},
volume = {32},
number = {14},
pages = {2627-2636},
year = {1998},
issn = {1352-2310},
doi = {https://doi.org/10.1016/S1352-2310(97)00447-0},
url = {https://www.sciencedirect.com/science/article/pii/S1352231097004470},
author = {M.W Gardner and S.R Dorling},
keywords = {Statistical modelling, neural network, backpropagation, artificial intelligence},
abstract = {Artificial neural networks are appearing as useful alternatives to traditional statistical modelling techniques in many scientific disciplines. This paper presents a general introduction and discussion of recent applications of the multilayer perceptron, one type of artificial neural network, in the atmospheric sciences.}
}
@article{RevModPhys.34.123,
  title = {The Perceptron: A Model for Brain Functioning. I},
  author = {Block, H. D.},
  journal = {Rev. Mod. Phys.},
  volume = {34},
  issue = {1},
  pages = {123--135},
  numpages = {0},
  year = {1962},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.34.123},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.34.123}
}
% This file was created with Citavi 6.15.2.0

@article{Rosenblatt.1958,
 abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
 author = {Rosenblatt, F.},
 year = {1958},
 title = {The perceptron: A probabilistic model for information storage and organization in the brain},
 keywords = {Brain;Cognition;Memory;Nervous System},
 pages = {386--408},
 volume = {65},
 number = {6},
 journal = {Psychological Review},
 doi = {10.1037/h0042519}
}
@article{Schmidhuber_2015,
	doi = {10.1016/j.neunet.2014.09.003},
	url = {https://doi.org/10.1016%2Fj.neunet.2014.09.003},
	year = 2015,
	month = {jan},
	publisher = {Elsevier {BV}},
	volume = {61},
	pages = {85--117},
	author = {Jürgen Schmidhuber},
	title = {Deep learning in neural networks: An overview},
	journal = {Neural Networks}
}
@misc{oshea2015introduction,
      title={An Introduction to Convolutional Neural Networks}, 
      author={Keiron O'Shea and Ryan Nash},
      year={2015},
      eprint={1511.08458},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@INPROCEEDINGS{8308186,
  author={Albawi, Saad and Mohammed, Tareq Abed and Al-Zawi, Saad},
  booktitle={2017 International Conference on Engineering and Technology (ICET)}, 
  title={Understanding of a convolutional neural network}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  doi={10.1109/ICEngTechnol.2017.8308186}
}
